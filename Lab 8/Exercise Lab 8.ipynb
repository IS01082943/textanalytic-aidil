{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86945e83-13dd-4cf0-bc19-88c69a4951d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = ''.join([word for word in text if not word.isdigit()])\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dfd0cdc-917e-4b94-8a90-fe02d5597ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I love playing football on the weekends\",\n",
    "    \"I enjoy hiking and camping in the mountains\",\n",
    "    \"I like to read books and watch movies\",\n",
    "    \"I prefer playing video games over sports\",\n",
    "    \"I love listening to music and going to concerts\"\n",
    "]\n",
    "preprocessed_dataset = [preprocess_text(doc) for doc in dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4a0b93-4cb6-4bed-b58d-3efdea3c8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b287c89-da7e-49df-8a50-67fecda46b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            0\n",
      "I enjoy hiking and camping in the mountains                        0\n",
      "I like to read books and watch movies                              0\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    0\n",
      "\n",
      "Purity: 0.8\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Display results\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Calculate purity (as per lab's method)\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"\\nPurity:\", purity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e182782f-a3ea-4b62-b8b2-256838db0f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        0\n",
      "I like to read books and watch movies                              1\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    0\n",
      "\n",
      "Purity: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preprocess and tokenize\n",
    "tokenized_dataset = [preprocess_text(doc).split() for doc in dataset]\n",
    "\n",
    "# Step 3: Train Word2Vec model\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_dataset,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "# Step 4: Create document embeddings\n",
    "X = np.array([\n",
    "    np.mean([word2vec_model.wv[word] for word in doc if word in word2vec_model.wv], axis=0)\n",
    "    for doc in tokenized_dataset\n",
    "])\n",
    "\n",
    "# Step 5: Perform clustering\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Display results\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"\\nPurity:\", purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36999090-a30d-42b0-bb46-c3df703d1e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 1 Results:\n",
      "TF-IDF Purity: 0.80\n",
      "Word2Vec Purity: 0.80\n",
      "\n",
      "\n",
      "Exercise 2 Results:\n",
      "\n",
      "TF-IDF Purity (Complaints): 0.58\n",
      "\n",
      "Top TF-IDF Terms per Cluster:\n",
      "Cluster 0:\n",
      "  contract\n",
      "  xfinity\n",
      "  signed\n",
      "  fee\n",
      "  second\n",
      "  know\n",
      "  year\n",
      "  rude\n",
      "  box\n",
      "  local\n",
      "\n",
      "Cluster 1:\n",
      "  service\n",
      "  internet\n",
      "  customer\n",
      "  comcast\n",
      "  would\n",
      "  mbps\n",
      "  speed\n",
      "  tech\n",
      "  month\n",
      "  cable\n",
      "\n",
      "Cluster 2:\n",
      "  rude\n",
      "  rep\n",
      "  day\n",
      "  service\n",
      "  call\n",
      "  helpful\n",
      "  overwhelming\n",
      "  cutting\n",
      "  pas\n",
      "  ignorant\n",
      "\n",
      "Word2Vec Purity (Complaints): 0.79\n",
      "\n",
      "Sample Complaints with Clusters:\n",
      "                                                text  cluster_tfidf  \\\n",
      "0  I used to love Comcast. Until all these consta...              1   \n",
      "1  I'm so over Comcast! The worst internet provid...              1   \n",
      "2  If I could give them a negative star or no sta...              1   \n",
      "\n",
      "   cluster_w2v  \n",
      "0            0  \n",
      "1            2  \n",
      "2            2  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- Exercise 1 + 2 Combined Solution -------------------------\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ------------------------- Text Preprocessing Function -------------------------\n",
    "def preprocess_text(text):\n",
    "    # Handle NaN values\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = ''.join([word for word in text if not word.isdigit()])\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# ------------------------- Exercise 1: TF-IDF & Word2Vec with Preprocessing -------------------------\n",
    "def exercise1():\n",
    "    # Sample dataset\n",
    "    dataset = [\n",
    "        \"I love playing football on the weekends\",\n",
    "        \"I enjoy hiking and camping in the mountains\",\n",
    "        \"I like to read books and watch movies\",\n",
    "        \"I prefer playing video games over sports\",\n",
    "        \"I love listening to music and going to concerts\"\n",
    "    ]\n",
    "\n",
    "    # Preprocess data\n",
    "    preprocessed_data = [preprocess_text(doc) for doc in dataset]\n",
    "\n",
    "    # ---------- TF-IDF ----------\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = vectorizer.fit_transform(preprocessed_data)\n",
    "    \n",
    "    # Clustering\n",
    "    k = 2\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(X_tfidf)\n",
    "    y_pred = km.predict(X_tfidf)\n",
    "    \n",
    "    # Calculate purity (lab's method)\n",
    "    cluster_counts = Counter(y_pred)\n",
    "    purity = max(cluster_counts.values()) / sum(cluster_counts.values())\n",
    "    print(f\"TF-IDF Purity: {purity:.2f}\")\n",
    "\n",
    "    # ---------- Word2Vec ----------\n",
    "    # Tokenize preprocessed data\n",
    "    tokenized_data = [doc.split() for doc in preprocessed_data]\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    model = Word2Vec(\n",
    "        sentences=tokenized_data,\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        workers=4\n",
    "    )\n",
    "    \n",
    "    # Create document embeddings\n",
    "    embeddings = []\n",
    "    for doc in tokenized_data:\n",
    "        valid_words = [word for word in doc if word in model.wv]\n",
    "        if len(valid_words) > 0:\n",
    "            embeddings.append(np.mean(model.wv[valid_words], axis=0))\n",
    "        else:\n",
    "            embeddings.append(np.zeros(model.vector_size))  # Handle empty docs\n",
    "    \n",
    "    # Clustering\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(embeddings)\n",
    "    y_pred = km.predict(embeddings)\n",
    "    \n",
    "    # Calculate purity\n",
    "    cluster_counts = Counter(y_pred)\n",
    "    purity = max(cluster_counts.values()) / sum(cluster_counts.values())\n",
    "    print(f\"Word2Vec Purity: {purity:.2f}\\n\")\n",
    "\n",
    "# ------------------------- Exercise 2: Customer Complaints Analysis -------------------------\n",
    "def exercise2():\n",
    "    # Load data\n",
    "    df = pd.read_csv('customer_complaints_1.csv')\n",
    "    texts = df['text'].fillna('').tolist()  # Handle NaN\n",
    "    \n",
    "    # Preprocess data\n",
    "    preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "    \n",
    "    # ---------- TF-IDF ----------\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(preprocessed_texts)\n",
    "    \n",
    "    # Clustering\n",
    "    k = 3\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(X_tfidf)\n",
    "    y_pred_tfidf = km.predict(X_tfidf)\n",
    "    \n",
    "    # Calculate purity\n",
    "    cluster_counts = Counter(y_pred_tfidf)\n",
    "    purity = max(cluster_counts.values()) / sum(cluster_counts.values())\n",
    "    print(f\"\\nTF-IDF Purity (Complaints): {purity:.2f}\")\n",
    "    \n",
    "    # Show top terms\n",
    "    print(\"\\nTop TF-IDF Terms per Cluster:\")\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    for i in range(k):\n",
    "        print(f\"Cluster {i}:\")\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(f\"  {terms[ind]}\")\n",
    "        print()\n",
    "\n",
    "    # ---------- Word2Vec ----------\n",
    "    # Tokenize data\n",
    "    tokenized_data = [doc.split() for doc in preprocessed_texts]\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    model = Word2Vec(\n",
    "        sentences=tokenized_data,\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        workers=4\n",
    "    )\n",
    "    \n",
    "    # Create embeddings\n",
    "    embeddings = []\n",
    "    for doc in tokenized_data:\n",
    "        valid_words = [word for word in doc if word in model.wv]\n",
    "        if len(valid_words) > 0:\n",
    "            embeddings.append(np.mean(model.wv[valid_words], axis=0))\n",
    "        else:\n",
    "            embeddings.append(np.zeros(model.vector_size))\n",
    "    \n",
    "    # Clustering\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(embeddings)\n",
    "    y_pred_w2v = km.predict(embeddings)\n",
    "    \n",
    "    # Calculate purity\n",
    "    cluster_counts = Counter(y_pred_w2v)\n",
    "    purity = max(cluster_counts.values()) / sum(cluster_counts.values())\n",
    "    print(f\"Word2Vec Purity (Complaints): {purity:.2f}\")\n",
    "    \n",
    "    # Add clusters to DataFrame\n",
    "    df['cluster_tfidf'] = y_pred_tfidf\n",
    "    df['cluster_w2v'] = y_pred_w2v\n",
    "    print(\"\\nSample Complaints with Clusters:\")\n",
    "    print(df[['text', 'cluster_tfidf', 'cluster_w2v']].head(3))\n",
    "\n",
    "# ------------------------- Execute Both Exercises -------------------------\n",
    "print(\"Exercise 1 Results:\")\n",
    "exercise1()\n",
    "\n",
    "print(\"\\nExercise 2 Results:\")\n",
    "exercise2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a555c4-b525-41f4-a17c-89a11818ddf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
