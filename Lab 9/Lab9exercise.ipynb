{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd7781ae-2118-4e74-98f9-f0c7ec5f635c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee843a8-aa86-4565-92b6-4e0237893bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\HP\\Downloads\\Lab 9\n",
      "Reading 'npr.csv'...\n",
      "CSV loaded successfully. Shape: (11992, 1)\n",
      "Loaded 11992 documents for processing\n",
      "First document sample (truncated):\n",
      "In the Washington of 2016, even when the policy can be bipartisan, the politics cannot. And in that ...\n",
      "Preprocessing documents...\n",
      "Preprocessed 11992 documents, 11992 non-empty\n",
      "Creating dictionary...\n",
      "Filtering dictionary: min_doc_count=15, max_ratio=0.5\n",
      "Dictionary created: 86155 original tokens, 15974 after filtering\n",
      "Creating document-term matrix...\n",
      "Created corpus with 11992 documents, 11992 non-empty\n",
      "Running LDA with 5 topics...\n",
      "LDA model trained successfully\n",
      "Determining dominant topic for each document...\n",
      "\n",
      "Results - Topic Distribution:\n",
      "Topic\n",
      "2    3686\n",
      "1    2633\n",
      "4    2045\n",
      "3    1903\n",
      "0    1725\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample of Results (first 5 rows):\n",
      "                                             Article  Topic\n",
      "0  In the Washington of 2016, even when the polic...      0\n",
      "1    Donald Trump has used Twitter  â€”   his prefe...      1\n",
      "2    Donald Trump is unabashedly praising Russian...      0\n",
      "3  Updated at 2:50 p. m. ET, Russian President Vl...      1\n",
      "4  From photography, illustration and video, to d...      2\n",
      "\n",
      "Top terms for each topic:\n",
      "\n",
      "Top terms for Topic #0:\n",
      "['trump', 'clinton', 'president', 'state', 'republican', 'campaign', 'election', 'vote', 'obama', 'voter']\n",
      "\n",
      "Top terms for Topic #1:\n",
      "['police', 'report', 'government', 'state', 'country', 'court', 'president', 'official', 'told', 'attack']\n",
      "\n",
      "Top terms for Topic #2:\n",
      "['know', 'think', 'thing', 'life', 'woman', 'really', 'story', 'show', 'book', 'day']\n",
      "\n",
      "Top terms for Topic #3:\n",
      "['food', 'water', 'world', 'human', 'scientist', 'disease', 'study', 'animal', 'science', 'university']\n",
      "\n",
      "Top terms for Topic #4:\n",
      "['health', 'school', 'child', 'student', 'percent', 'company', 'state', 'care', 'woman', 'program']\n",
      "\n",
      "Detailed weights for each topic:\n",
      "Topic 0:\n",
      "- \"trump\" (weight: 0.033)\n",
      "- \"clinton\" (weight: 0.013)\n",
      "- \"president\" (weight: 0.010)\n",
      "- \"state\" (weight: 0.009)\n",
      "- \"republican\" (weight: 0.009)\n",
      "- \"campaign\" (weight: 0.009)\n",
      "- \"election\" (weight: 0.007)\n",
      "- \"vote\" (weight: 0.006)\n",
      "- \"obama\" (weight: 0.006)\n",
      "- \"voter\" (weight: 0.005)\n",
      "\n",
      "Topic 1:\n",
      "- \"police\" (weight: 0.006)\n",
      "- \"report\" (weight: 0.006)\n",
      "- \"government\" (weight: 0.005)\n",
      "- \"state\" (weight: 0.005)\n",
      "- \"country\" (weight: 0.005)\n",
      "- \"court\" (weight: 0.005)\n",
      "- \"president\" (weight: 0.004)\n",
      "- \"official\" (weight: 0.004)\n",
      "- \"told\" (weight: 0.004)\n",
      "- \"attack\" (weight: 0.004)\n",
      "\n",
      "Topic 2:\n",
      "- \"know\" (weight: 0.005)\n",
      "- \"think\" (weight: 0.005)\n",
      "- \"thing\" (weight: 0.005)\n",
      "- \"life\" (weight: 0.005)\n",
      "- \"woman\" (weight: 0.004)\n",
      "- \"really\" (weight: 0.004)\n",
      "- \"story\" (weight: 0.004)\n",
      "- \"show\" (weight: 0.003)\n",
      "- \"book\" (weight: 0.003)\n",
      "- \"day\" (weight: 0.003)\n",
      "\n",
      "Topic 3:\n",
      "- \"food\" (weight: 0.007)\n",
      "- \"water\" (weight: 0.005)\n",
      "- \"world\" (weight: 0.004)\n",
      "- \"human\" (weight: 0.003)\n",
      "- \"scientist\" (weight: 0.003)\n",
      "- \"disease\" (weight: 0.003)\n",
      "- \"study\" (weight: 0.003)\n",
      "- \"animal\" (weight: 0.003)\n",
      "- \"science\" (weight: 0.003)\n",
      "- \"university\" (weight: 0.003)\n",
      "\n",
      "Topic 4:\n",
      "- \"health\" (weight: 0.009)\n",
      "- \"school\" (weight: 0.008)\n",
      "- \"child\" (weight: 0.006)\n",
      "- \"student\" (weight: 0.006)\n",
      "- \"percent\" (weight: 0.005)\n",
      "- \"company\" (weight: 0.005)\n",
      "- \"state\" (weight: 0.005)\n",
      "- \"care\" (weight: 0.005)\n",
      "- \"woman\" (weight: 0.004)\n",
      "- \"program\" (weight: 0.004)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Add error handling to print helpful information\n",
    "try:\n",
    "    # Download necessary NLTK resources with quiet option to reduce output\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    \n",
    "    # Print current working directory to see where Python is looking for the file\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Check if file exists before attempting to read\n",
    "    if not os.path.exists('npr.csv'):\n",
    "        print(\"Error: 'npr.csv' file not found in the current directory!\")\n",
    "        print(\"Please make sure you've uploaded the file and it's in the correct location.\")\n",
    "        # If you're using an interactive environment like Google Colab, you might need to upload the file\n",
    "        # or provide a sample dataset for testing\n",
    "        \n",
    "        # Example data for testing if file is not available\n",
    "        print(\"Using example data for demonstration...\")\n",
    "        documents = [\n",
    "            \"Rafael Nadal Joins Roger Federer in Missing U.S. Open\",\n",
    "            \"Rafael Nadal Is Out of the Australian Open\",\n",
    "            \"Biden Announces Virus Measures\",\n",
    "            \"Biden's Virus Plans Meet Reality\",\n",
    "            \"Where Biden's Virus Plan Stands\"\n",
    "        ]\n",
    "    else:\n",
    "        # Read the CSV file\n",
    "        print(\"Reading 'npr.csv'...\")\n",
    "        df = pd.read_csv('npr.csv')\n",
    "        print(f\"CSV loaded successfully. Shape: {df.shape}\")\n",
    "        \n",
    "        # Check if 'Article' column exists\n",
    "        if 'Article' not in df.columns:\n",
    "            print(f\"Error: 'Article' column not found in CSV. Available columns: {df.columns.tolist()}\")\n",
    "            raise KeyError(\"'Article' column not found in the CSV file\")\n",
    "            \n",
    "        # Check for null values in the Article column\n",
    "        null_count = df['Article'].isnull().sum()\n",
    "        if null_count > 0:\n",
    "            print(f\"Warning: Found {null_count} null values in 'Article' column\")\n",
    "            df = df.dropna(subset=['Article'])\n",
    "            print(f\"Dropped null values. New shape: {df.shape}\")\n",
    "            \n",
    "        # Convert Article column to list\n",
    "        documents = df['Article'].tolist()\n",
    "        print(f\"Loaded {len(documents)} documents for processing\")\n",
    "        \n",
    "        # Print first document sample\n",
    "        if len(documents) > 0:\n",
    "            print(\"First document sample (truncated):\")\n",
    "            print(documents[0][:100] + \"...\" if len(documents[0]) > 100 else documents[0])\n",
    "    \n",
    "    # Initialize preprocessing tools\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def preprocess_text(text):\n",
    "        \"\"\"Process a single text document\"\"\"\n",
    "        try:\n",
    "            # Make sure text is a string\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)\n",
    "                \n",
    "            # Tokenize and preprocess\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            tokens = [token for token in tokens if token.isalnum()]\n",
    "            tokens = [token for token in tokens if token not in stop_words]\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "            return tokens\n",
    "        except Exception as e:\n",
    "            print(f\"Error preprocessing text: {str(e)}\")\n",
    "            print(f\"Problematic text (truncated): {str(text)[:50]}...\")\n",
    "            return []  # Return empty list for problematic documents\n",
    "    \n",
    "    print(\"Preprocessing documents...\")\n",
    "    preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "    non_empty_count = sum(1 for doc in preprocessed_documents if len(doc) > 0)\n",
    "    print(f\"Preprocessed {len(preprocessed_documents)} documents, {non_empty_count} non-empty\")\n",
    "    \n",
    "    if non_empty_count == 0:\n",
    "        raise ValueError(\"All documents are empty after preprocessing!\")\n",
    "    \n",
    "    # Create dictionary and filter\n",
    "    print(\"Creating dictionary...\")\n",
    "    dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "    original_tokens = len(dictionary)\n",
    "    \n",
    "    # Adjust filter values based on corpus size\n",
    "    min_doc_count = min(15, len(documents) // 10) if len(documents) > 20 else 2\n",
    "    print(f\"Filtering dictionary: min_doc_count={min_doc_count}, max_ratio=0.5\")\n",
    "    \n",
    "    dictionary.filter_extremes(no_below=min_doc_count, no_above=0.5)\n",
    "    filtered_tokens = len(dictionary)\n",
    "    print(f\"Dictionary created: {original_tokens} original tokens, {filtered_tokens} after filtering\")\n",
    "    \n",
    "    if filtered_tokens == 0:\n",
    "        raise ValueError(\"Dictionary is empty after filtering! Try adjusting filter parameters.\")\n",
    "    \n",
    "    # Create corpus\n",
    "    print(\"Creating document-term matrix...\")\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
    "    non_empty_corpus = sum(1 for doc in corpus if len(doc) > 0)\n",
    "    print(f\"Created corpus with {len(corpus)} documents, {non_empty_corpus} non-empty\")\n",
    "    \n",
    "    if non_empty_corpus == 0:\n",
    "        raise ValueError(\"Corpus is empty! All documents filtered out.\")\n",
    "    \n",
    "    # Run LDA with appropriate number of topics\n",
    "    n_topics = min(5, len(documents) // 5) if len(documents) > 10 else 2\n",
    "    print(f\"Running LDA with {n_topics} topics...\")\n",
    "    lda_model = LdaModel(corpus, num_topics=n_topics, id2word=dictionary, passes=15)\n",
    "    print(\"LDA model trained successfully\")\n",
    "    \n",
    "    # Process results\n",
    "    print(\"Determining dominant topic for each document...\")\n",
    "    article_labels = []\n",
    "    for doc in preprocessed_documents:\n",
    "        bow = dictionary.doc2bow(doc)\n",
    "        if len(bow) > 0:  # Only process non-empty documents\n",
    "            topics = lda_model.get_document_topics(bow)\n",
    "            dominant_topic = max(topics, key=lambda x: x[1])[0] if topics else -1\n",
    "        else:\n",
    "            dominant_topic = -1  # Mark empty documents\n",
    "        article_labels.append(dominant_topic)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    df_result = pd.DataFrame({\"Article\": documents, \"Topic\": article_labels})\n",
    "    print(\"\\nResults - Topic Distribution:\")\n",
    "    print(df_result[\"Topic\"].value_counts())\n",
    "    \n",
    "    # Show sample of results\n",
    "    print(\"\\nSample of Results (first 5 rows):\")\n",
    "    pd.set_option('display.max_colwidth', 50)  # Limit column width for display\n",
    "    print(df_result.head())\n",
    "    \n",
    "    # Show top terms for each topic\n",
    "    print(\"\\nTop terms for each topic:\")\n",
    "    for topic_id in range(lda_model.num_topics):\n",
    "        print(f\"\\nTop terms for Topic #{topic_id}:\")\n",
    "        print([term[0] for term in lda_model.show_topic(topic_id, topn=10)])\n",
    "    \n",
    "    # Show detailed weights\n",
    "    print(\"\\nDetailed weights for each topic:\")\n",
    "    for idx, topic in lda_model.print_topics():\n",
    "        print(f\"Topic {idx}:\")\n",
    "        terms = [term.strip() for term in topic.split(\"+\")]\n",
    "        for term in terms:\n",
    "            weight, word = term.split(\"*\")\n",
    "            print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "        print()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060c59dc-da89-4fdc-935e-cddc1034e49c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
